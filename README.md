
# ğŸ§  RAG + PPO: Reinforcement Learning Meets Retrieval in Social Dilemmas

## ğŸ“Œ Overview
This project explores a hybrid RAG (Retrieval-Augmented Generation) and PPO (Proximal Policy Optimization) approach to teach LLM-based agents socially adaptive behavior in the **Iterated Prisoner's Dilemma** â€” a classic setting for studying trust, cooperation, and betrayal.

## ğŸ§  Motivation
Traditional reinforcement learning agents struggle with **human-like reasoning** in social games. By introducing **retrieval mechanisms**, we allow agents to reference **past behaviors** and adapt more intelligently â€” similar to how humans learn from memory.

---

## âš™ï¸ Architecture

### âœ… Components:
- **Environment**: Iterated Prisoner's Dilemma (IPD)
- **Agent**: Language model (LLM-based policy)
- **Retriever**: Fetches similar behavioral episodes
- **History Buffer**: Stores `(opponent policy, moves, rewards)`
- **PPO Trainer**: Optimizes the agentâ€™s strategy over time

### ğŸ”„ Workflow:
1. Agent plays repeated rounds of IPD.
2. Every *N* steps:
   - Retrieve top-*k* similar past episodes.
   - Combine with current observation as input.
3. Agent generates next action (Cooperate/Defect).
4. Environment returns reward.
5. PPO updates the policy based on total episodic return.

---

## ğŸ§ª Evaluation: Fixed Opponent Benchmarks

| Opponent Policy        | Description                              | PPO Behavior Summary             | Success |
|------------------------|------------------------------------------|----------------------------------|---------|
| Tit for Tat            | Cooperates, mimics last move             | Learned to cooperate             | âœ…      |
| Always Defect          | Always defects                           | Learned to defect in return      | âœ…      |
| Always Cooperate       | Always cooperates                        | Exploited with defect            | âœ…      |
| Friedman               | Punishes once defected                   | Early defection triggered loss   | âŒ      |
| Joss                   | 90% cooperation, 10% random defect       | Mostly cooperative               | âœ…      |
| Tester                 | Cooperates, then defects permanently     | Detected pattern, adapted        | âœ…      |
| Backstabber            | Cooperates 5 rounds, then defects always | Delayed detection, adapted late  | âœ…      |

---

## ğŸ’¡ Why It Matters

- ğŸ§  **Memory-Augmented Decision-Making**: Simulates how humans use memory + feedback.
- ğŸ“š **Education Modeling**: Extendable to student-agent modeling based on thinking patterns.
- ğŸ¤– **Social Agent Foundations**: Step toward adaptive, context-aware, LLM-powered agents in dynamic interactions.

---

## ğŸ› ï¸ Future Directions
- Expand to 1v1 adaptive LLM vs LLM agents
- Fine-tune models on specific interaction traits (e.g., trust, retaliation)
- Visualize evolving policy strategies over time

---

## ğŸ“ Folder Structure
rag-ppo-ipd/
â”œâ”€â”€ agents/ # LLM policy + retriever wrapper
â”œâ”€â”€ env/ # Prisoner's Dilemma environment
â”œâ”€â”€ trainer/ # PPO implementation
â”œâ”€â”€ data/ # Episode history buffer
â”œâ”€â”€ notebooks/ # Experiments and visualizations
â””â”€â”€ README.md # You are here ğŸš€

yaml
Copy
Edit

---

## ğŸ“œ Citation / Inspiration
If you're inspired by this work, feel free to fork, cite, or reach out!


